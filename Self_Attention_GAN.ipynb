{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-Attention-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nnajeh/GAN/blob/main/Self_Attention_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFKV4ZlIVfS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd36df52-59ad-4986-cda7-b8d65788c20b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-tdFi-3bV6Pv"
      },
      "source": [
        "#@title Import Libraries\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFBtkzkXHOOn"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import autograd, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torchvision.datasets as dataset\n",
        "import torchvision.datasets \n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable \n",
        "#import spectral \n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjZnM_bfVFup"
      },
      "source": [
        "from torchvision.utils import save_image\n",
        "from IPython.display import clear_output\n",
        "import torch.nn as nn\n",
        "import datetime\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZWEXTQvEvcS"
      },
      "source": [
        "batch_size =16"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLu_0sAgEdcY"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ilTOGMH0Vi92"
      },
      "source": [
        "#@title Define Dataset"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ7ErTzOD1eR"
      },
      "source": [
        "# Utility function for help of arranging sample images\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp_(0, 1)\n",
        "    "
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYXcXzZDD9B5"
      },
      "source": [
        "# random noise\n",
        "fixed_z = torch.randn(16,100).to(device)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im6_XYVJL8Iy"
      },
      "source": [
        "data_root = \"./data/\"\n",
        "\n",
        "train_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(256),\n",
        "                                      transforms.Grayscale(1),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,),(0.5,))\n",
        "])\n",
        "                                      \n",
        "dataset_train = dataset.ImageFolder(data_root + 'train', train_transform)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=dataset_train,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True,\n",
        "                             drop_last=True)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bMIv6TwGJkn"
      },
      "source": [
        "#Construct the Self-Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI6PL9WBDnsJ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Construct the conv layers\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        \n",
        "        # Initialize gamma as 0\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax  = nn.Softmax(dim=-1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B * C * W * H)\n",
        "            returns :\n",
        "                out : self attention value + input feature \n",
        "                attention: B * N * N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        \n",
        "        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0,2,1) # B * N * C\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n",
        "        energy =  torch.bmm(proj_query, proj_key) # batch matrix-matrix product\n",
        "        \n",
        "        attention = self.softmax(energy) # B * N * N\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n",
        "        out = torch.bmm(proj_value, attention.permute(0,2,1)) # batch matrix-matrix product\n",
        "        out = out.view(m_batchsize,C,width,height) # B * C * W * H\n",
        "        \n",
        "        # Add attention weights onto input\n",
        "        out = self.gamma*out + x\n",
        "        return out, attention"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00W_r6kTOK7"
      },
      "source": [
        "#Spectral Normalization is a novel weight normalization technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFcbWeKkXYeR"
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "        "
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhCMEck6IUEe"
      },
      "source": [
        "# GAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McAoemxuTQqY"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generator\n",
        "    input: \n",
        "        z: latent matrix with shape of (batch_size, 100)\n",
        "    output: \n",
        "        out: generated image with shape (batch_size, 1, 256, 256)\n",
        "        p1: attention matrix generated by attn layer\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size=16, attn=True, image_size=256, z_dim=100, conv_dim=64):\n",
        "        super().__init__()\n",
        "        self.attn = attn\n",
        "        \n",
        "        # Layer 1 turn 100 dims -> 512 dims, size 1 -> 3\n",
        "        layer1 = []\n",
        "        layer1.append(SpectralNorm(nn.ConvTranspose2d(in_channels = z_dim, out_channels = conv_dim*8, kernel_size = 3)))\n",
        "        layer1.append(nn.BatchNorm2d(conv_dim*8))\n",
        "        layer1.append(nn.ReLU())\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        \n",
        "        layer2 = []\n",
        "        layer2.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*8, out_channels = conv_dim*4, \n",
        "                                                      kernel_size = 3, stride = 2, padding = 0)))\n",
        "        layer2.append(nn.BatchNorm2d(conv_dim*4))\n",
        "        layer2.append(nn.ReLU())\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        \n",
        "        layer3 = []\n",
        "        layer3.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*4, out_channels = conv_dim*2, \n",
        "                                                      kernel_size = 4, stride = 2, padding = 1)))\n",
        "        layer3.append(nn.BatchNorm2d(conv_dim*2))\n",
        "        layer3.append(nn.ReLU())\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        self.attn = Self_Attn(conv_dim*2)\n",
        "        \n",
        "        last = []\n",
        "        last.append(nn.ConvTranspose2d(conv_dim*2, 1, 4, 2, 1))\n",
        "        last.append(nn.Tanh())\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z is the input random matrix for generator\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        out=self.l1(z)\n",
        "        out=self.l2(out)\n",
        "        out=self.l3(out)\n",
        "        \n",
        "        if self.attn == True:\n",
        "            out = self.attn(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtLbTm17N-E7"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminator\n",
        "    input:\n",
        "        x: one batch of data with shape of (batch_size, 1, 256, 256)\n",
        "    output: \n",
        "        out.squeeze: a batch of scalars indicating the predict results\n",
        "        p1: attention matrix generated by attn layer\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size=16, attn=True, image_size=256, conv_dim=64):\n",
        "        super().__init__()\n",
        "        self.attn = attn\n",
        "        \n",
        "        curr_dim = conv_dim\n",
        "        \n",
        "        layer1 = []\n",
        "        layer1.append(SpectralNorm(nn.Conv2d(1, conv_dim, 4, 2, 1)))\n",
        "        layer1.append(nn.LeakyReLU(0.1))\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        \n",
        "        layer2 = []\n",
        "        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer2.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        \n",
        "        layer3 = []\n",
        "        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer3.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "        \n",
        "        self.attn = Self_Attn(curr_dim)\n",
        "        \n",
        "        last = []\n",
        "        last.append(nn.Conv2d(curr_dim, 1, 4, 2, 1))\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.l2(out)\n",
        "        out = self.l3(out)\n",
        "\n",
        "        if self.attn == True:\n",
        "            out = self.attn(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out.squeeze()"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR_X3vCDYJXC"
      },
      "source": [
        "epochs = 1000\n",
        "batch_size = 16\n",
        "z_dim = 100\n",
        "attn = True"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGS2KE5sU0Ec"
      },
      "source": [
        "batches_done = 0\n",
        "sample_interval = 10"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRfrpLuSKcyJ"
      },
      "source": [
        "#Display generated data \n",
        "def image_check(gen_fake):\n",
        "    img = gen_fake.data.numpy()\n",
        "    for i in range(2):\n",
        "        plt.imshow(img[i][0],cmap='gray')\n",
        "        plt.show()"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIY9yS5OIhmO"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGJQLO145jqD"
      },
      "source": [
        "from torchvision.utils import save_image\n",
        "from IPython.display import clear_output\n",
        "import torch.nn as nn\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train(epochs = 1000, batch_size = 16, z_dim = 100, attn = True):\n",
        "\n",
        "  # Initialize model\n",
        "  G = Generator(batch_size, attn).to(device)\n",
        "  D = Discriminator(batch_size, attn).to(device)\n",
        "    \n",
        "  # Make directory for samples and models\n",
        "  cwd = os.getcwd()\n",
        "\n",
        "  post='_attn' if attn else ''\n",
        "\n",
        "  if not os.path.exists(cwd+'/images'+post):\n",
        "    os.makedirs(cwd+'/images'+post)\n",
        "\n",
        "  # Initialize optimizer with filter, lr and coefficients\n",
        "  g_optimizer = torch.optim.Adam(G.parameters(), 0.0001, [0.0,0.9])\n",
        "  d_optimizer = torch.optim.Adam(D.parameters(), 0.0004, [0.0,0.9])\n",
        "    \n",
        "\n",
        "  # Start timer\n",
        "  start_time = time.time()\n",
        "    \n",
        "  for epoch in range(epochs):\n",
        "    D.train()\n",
        "    G.train()\n",
        "\n",
        "    for i, (Iter,_) in enumerate(train_dataloader):   \n",
        "        \n",
        "        #================= Train D ================== #\n",
        "        \n",
        "        real_images= Iter.to(device)\n",
        "\n",
        " #       try:\n",
        " #           real_images,_ = next(Iter)\n",
        "  #      except:\n",
        "   #         Iter = iter(train_dataloader)\n",
        "    #        real_images,_ = next(Iter)\n",
        "        \n",
        "        # Compute loss with real images\n",
        "        d_out_real = D(real_images.to(device))\n",
        "        d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\n",
        "        \n",
        "        # Compute loss with fake images\n",
        "        z = torch.randn(batch_size, z_dim).to(device)\n",
        "\n",
        "        fake_images = G(z)\n",
        "        d_out_fake = D(fake_images)\n",
        "        d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\n",
        "        \n",
        "        # Backward + Optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        d_optimizer.zero_grad(); \n",
        "        g_optimizer.zero_grad()\n",
        "        \n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        # ================== Train G ================== #\n",
        "        # Create random noise\n",
        "        z = torch.randn(batch_size, z_dim).to(device)\n",
        "        fake_images = G(z)\n",
        "        g_out_fake = D(fake_images)\n",
        "        g_loss_fake = - g_out_fake.mean()\n",
        "        \n",
        "        d_optimizer.zero_grad(); \n",
        "        g_optimizer.zero_grad()\n",
        "        \n",
        "        g_loss_fake.backward()\n",
        "        g_optimizer.step()\n",
        "        \n",
        "       # accuracy = accuracy_score(fake_images.mean(),real_images.mean())\n",
        "      #  print(\"accuracy: %.2f\" %acc)\n",
        "\n",
        "        # Print out log info\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            expect = elapsed/(epoch + 1)*(epochs-epoch-1)\n",
        "            \n",
        "            elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "\n",
        "            expect = str(datetime.timedelta(seconds=expect))\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            print(\"Elapsed [{}], Expect [{}], epoch [{}/{}], D_real_loss: {:.4f}, G_real_losss: {:.4f}, ave_generator_gamma: {:.4f}\"\n",
        "                   .format(elapsed,\n",
        "                           expect,\n",
        "                           epoch + 1,\n",
        "                           epochs,\n",
        "                           d_loss_real.item(),\n",
        "                           g_loss_fake.item(),\n",
        "                           G.attn.gamma.mean().item())\n",
        "                  )\n",
        "                                    \n",
        "        if (epoch + 1) % (100) == 0:\n",
        "                save_image(fake_images.data[:25],f\"./images/{batches_done:06}.png\", nrow=5, normalize=True)\n",
        "                        \n",
        "        image_check(fake_images.cpu())\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkElf8ILVx_2"
      },
      "source": [
        "train(epochs =1000, batch_size = 16, z_dim = 100, attn = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1qgS05SQLPk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}